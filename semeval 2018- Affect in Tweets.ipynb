{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"semeval 2018- Affect in Tweets.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"68fb1412354242559c4478416697dfc3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_394baa893b4f4b13b26c4f805aaaf896","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8363715b02584a85a1caf73083184cc5","IPY_MODEL_3fd55cbe591b4707ae8eb84cd3d2aef4"]}},"394baa893b4f4b13b26c4f805aaaf896":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8363715b02584a85a1caf73083184cc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f26c17623e234fffac749533ea2b2ef6","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8bc0004c5d4e44c6a426b0b52e7d3bdb"}},"3fd55cbe591b4707ae8eb84cd3d2aef4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_57826cd4f0fb4843905f52b2ed321001","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 3.83MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e0b56daba64441e3a8dd89ff2a7337cc"}},"f26c17623e234fffac749533ea2b2ef6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8bc0004c5d4e44c6a426b0b52e7d3bdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57826cd4f0fb4843905f52b2ed321001":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e0b56daba64441e3a8dd89ff2a7337cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4814781977d64b79a5a9bdd0c4483202":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c2bb8f94141d4fcc8eba4d20da95cc2c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c7080f4eb5f443eeaafb530133811a37","IPY_MODEL_d970d26cd75544d4be1760d5b49aa3a1"]}},"c2bb8f94141d4fcc8eba4d20da95cc2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7080f4eb5f443eeaafb530133811a37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_25b871a39c924b578125e78e3bebdeee","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11e7763d35964c99aa8e360c945d3508"}},"d970d26cd75544d4be1760d5b49aa3a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_81161888700c4afba0098dfaf15024c6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 3.88MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9183a8a37964308a4f6569584879cca"}},"25b871a39c924b578125e78e3bebdeee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"11e7763d35964c99aa8e360c945d3508":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"81161888700c4afba0098dfaf15024c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b9183a8a37964308a4f6569584879cca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"093dfc906764480392e22226e7f93603":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8213ea3a75fa4f3089859510877dfca6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_106d39e0ae0146ce8078a1a6a867496b","IPY_MODEL_1911a4627a804d4ab7bcd61bc419cbbf"]}},"8213ea3a75fa4f3089859510877dfca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"106d39e0ae0146ce8078a1a6a867496b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8c1af60beec345719bc97f9e8007377f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d02da7f46f9442cacf0582e2ca6c7da"}},"1911a4627a804d4ab7bcd61bc419cbbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_71b930d2648f4dd7bf57b34f07147763","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:00&lt;00:00, 7.77kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae3071d8004d452ab2fb4ec754274eab"}},"8c1af60beec345719bc97f9e8007377f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0d02da7f46f9442cacf0582e2ca6c7da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"71b930d2648f4dd7bf57b34f07147763":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ae3071d8004d452ab2fb4ec754274eab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2df6fae69c241a5bb6bbbb923867e2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3c336c1c72ef41fd9f3eea14d141f58e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7a32aab41dd44df2b14d44df4aabf408","IPY_MODEL_056d08781c6048d88cbc08868a5e5b5b"]}},"3c336c1c72ef41fd9f3eea14d141f58e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a32aab41dd44df2b14d44df4aabf408":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_80b9c4a9fef24b23b52dba3e87e4c660","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":657434796,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":657434796,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf469c3b35394ccea5a6de1bc1b3b9c7"}},"056d08781c6048d88cbc08868a5e5b5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f4c5dd75f001477b924044d952a3f14b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 657M/657M [00:21&lt;00:00, 31.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce999a357bd347f398c3e6111147aefb"}},"80b9c4a9fef24b23b52dba3e87e4c660":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bf469c3b35394ccea5a6de1bc1b3b9c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f4c5dd75f001477b924044d952a3f14b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce999a357bd347f398c3e6111147aefb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"4j_5-q2crdw6"},"source":["#SemEval 2018 - Affect in Tweets\r\n","https://competitions.codalab.org/competitions/17751\r\n","http://saifmohammad.com/WebDocs/semeval2018-task1.pdf\r\n","#### Task E-c: Detecting Emotions (multi-label classification) -- This is a traditional Emotion Classification Task\r\n","\r\n","**Given:**\r\n","*   a tweet\r\n","\r\n","**Task:** classify the tweet as 'neutral or no emotion' or as one, or more, of eleven given emotions that best represent the mental state of the tweeter:\r\n","\r\n","*   anger (also includes annoyance and rage) \r\n","*   anticipation (also includes interest and vigilance) \r\n","*   disgust (also includes disinterest, dislike and loathing)\r\n","*   fear (also includes apprehension, anxiety, concern, and terror) \r\n","*   joy (also includes serenity and ecstasy) \r\n","*   love (also includes affection) \r\n","*   optimism (also includes hopefulness and confidence) \r\n","*   pessimism (also includes cynicism and lack of confidence) \r\n","*   sadness (also includes pensiveness and grief) \r\n","*   suprise (also includes distraction and amazement) \r\n","*   trust (also includes acceptance, liking, and admiration) \r\n","\r\n","**Data was downloaded into google drive from:**\r\n","https://competitions.codalab.org/competitions/17751#learn_the_details-datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvgqdTd_EVzf","executionInfo":{"status":"ok","timestamp":1611684711154,"user_tz":420,"elapsed":26010,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"fe49a505-6ee7-4109-bb60-be1a8b0f6d3a"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mdYkiHHbOyH9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611684781457,"user_tz":420,"elapsed":20430,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"b54c7d1b-0b21-4447-b935-8572e18e3858"},"source":["!pip install -q transformers==3.5.1\n","!pip install -q tf-models-official==2.3.0\n","!pip install -q emojis\n","!pip install -q -U sklearn"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 18.1MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 52.8MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 57.3MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 52.5MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 849kB 16.0MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102kB 14.8MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37.6MB 85kB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 358kB 57.2MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 174kB 64.6MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3xELdwPwO5dE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611684797718,"user_tz":420,"elapsed":7575,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"058e337a-07b2-4b37-fb5f-9379f9c68a20"},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from transformers import TFRobertaModel, RobertaTokenizer\n","from unicodedata import normalize\n","import emojis\n","import re\n","from sklearn import metrics \n","from sklearn.utils import class_weight\n","\n","print(\"TF Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(tf.config.list_physical_devices('GPU'))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TF Version:  2.4.0\n","Eager mode:  True\n","[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vgU4S5-eBbn7","colab":{"base_uri":"https://localhost:8080/","height":318,"referenced_widgets":["68fb1412354242559c4478416697dfc3","394baa893b4f4b13b26c4f805aaaf896","8363715b02584a85a1caf73083184cc5","3fd55cbe591b4707ae8eb84cd3d2aef4","f26c17623e234fffac749533ea2b2ef6","8bc0004c5d4e44c6a426b0b52e7d3bdb","57826cd4f0fb4843905f52b2ed321001","e0b56daba64441e3a8dd89ff2a7337cc","4814781977d64b79a5a9bdd0c4483202","c2bb8f94141d4fcc8eba4d20da95cc2c","c7080f4eb5f443eeaafb530133811a37","d970d26cd75544d4be1760d5b49aa3a1","25b871a39c924b578125e78e3bebdeee","11e7763d35964c99aa8e360c945d3508","81161888700c4afba0098dfaf15024c6","b9183a8a37964308a4f6569584879cca","093dfc906764480392e22226e7f93603","8213ea3a75fa4f3089859510877dfca6","106d39e0ae0146ce8078a1a6a867496b","1911a4627a804d4ab7bcd61bc419cbbf","8c1af60beec345719bc97f9e8007377f","0d02da7f46f9442cacf0582e2ca6c7da","71b930d2648f4dd7bf57b34f07147763","ae3071d8004d452ab2fb4ec754274eab","d2df6fae69c241a5bb6bbbb923867e2f","3c336c1c72ef41fd9f3eea14d141f58e","7a32aab41dd44df2b14d44df4aabf408","056d08781c6048d88cbc08868a5e5b5b","80b9c4a9fef24b23b52dba3e87e4c660","bf469c3b35394ccea5a6de1bc1b3b9c7","f4c5dd75f001477b924044d952a3f14b","ce999a357bd347f398c3e6111147aefb"]},"executionInfo":{"status":"ok","timestamp":1611684829840,"user_tz":420,"elapsed":24923,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"581576a5-b2ff-4585-ba2c-3a0c6bfcc1e7"},"source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_layer = TFRobertaModel.from_pretrained('roberta-base')"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68fb1412354242559c4478416697dfc3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti‚Ä¶"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4814781977d64b79a5a9bdd0c4483202","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti‚Ä¶"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"093dfc906764480392e22226e7f93603","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_‚Ä¶"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2df6fae69c241a5bb6bbbb923867e2f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=657434796.0, style=ProgressStyle(descri‚Ä¶"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Z0YPLgAKtxxK"},"source":["### Read Data"]},{"cell_type":"code","metadata":{"id":"19foOkonRWRw","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1611684849621,"user_tz":420,"elapsed":1466,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"2276eb65-7090-4520-84eb-d8a11a541759"},"source":["trainDF = pd.read_csv('gdrive/My Drive/Sentiment Discovery/emotions_data/2018-E-c-En-train.txt',delimiter='\\t')\n","devDF = pd.read_csv('gdrive/My Drive/Sentiment Discovery/emotions_data/2018-E-c-En-dev.txt',delimiter='\\t')\n","testDF = pd.read_csv('gdrive/My Drive/Sentiment Discovery/emotions_data/2018-E-c-En-test-gold.txt',delimiter='\\t')\n","\n","trainDF.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Tweet</th>\n","      <th>anger</th>\n","      <th>anticipation</th>\n","      <th>disgust</th>\n","      <th>fear</th>\n","      <th>joy</th>\n","      <th>love</th>\n","      <th>optimism</th>\n","      <th>pessimism</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>trust</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2017-En-21441</td>\n","      <td>‚ÄúWorry is a down payment on a problem you may ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2017-En-31535</td>\n","      <td>Whatever you decide to do make sure it makes y...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2017-En-21068</td>\n","      <td>@Max_Kellerman  it also helps that the majorit...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2017-En-31436</td>\n","      <td>Accept the challenges so that you can literall...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2017-En-22195</td>\n","      <td>My roommate: it's okay that we can't spell bec...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              ID  ... trust\n","0  2017-En-21441  ...     1\n","1  2017-En-31535  ...     0\n","2  2017-En-21068  ...     0\n","3  2017-En-31436  ...     0\n","4  2017-En-22195  ...     0\n","\n","[5 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"_lrUm7-Ot8VA"},"source":["### Print a few examples"]},{"cell_type":"code","metadata":{"id":"yXxtxQaw3ZQj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611684852249,"user_tz":420,"elapsed":306,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"c60ab32e-e44f-4b19-f4db-a5ec96aa2e3c"},"source":["for a in trainDF['Tweet'][:5]:\n","  print(a)\n","  print('-'*50)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["‚ÄúWorry is a down payment on a problem you may never have'. ¬†Joyce Meyer.  #motivation #leadership #worry\n","--------------------------------------------------\n","Whatever you decide to do make sure it makes you #happy.\n","--------------------------------------------------\n","@Max_Kellerman  it also helps that the majority of NFL coaching is inept. Some of Bill O'Brien's play calling was wow, ! #GOPATS\n","--------------------------------------------------\n","Accept the challenges so that you can literally even feel the exhilaration of victory.' -- George S. Patton üê∂\n","--------------------------------------------------\n","My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs\n","--------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pOtiScQRuDEV"},"source":["### Clean Data"]},{"cell_type":"code","metadata":{"id":"iFlTv_Ab7SJz","executionInfo":{"status":"ok","timestamp":1611684856831,"user_tz":420,"elapsed":258,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["def clean_tweet(s):\n","    \"\"\" Accepts a tweet and cleans data for deep learning\"\"\"\n","    s = normalize('NFKD',s) # remove weird encodings\n","    s = s.replace('#','') # remove hashtags\n","    s = emojis.decode(s) # decode emojis\n","    s = re.sub('@[^\\s]+','',s) # remove username\n","    s = s.lower() # convert to lower case\n","    return s\n","\n","def df_to_bert(df,max_seq_len,model=True):\n","    \"\"\" Input: \n","        df - a dataframe with Tweet as the sentence column and the anget to trust as the 11 emotion columns.\n","        max_seq_len - maximum number of tokens in the sentence. Pad with zeros if Tweet is too short otherwise truncate to max_seq_len\n","        model - True/False whether to create an output label or not\n","    \"\"\"\n","    output_dict = tokenizer([clean_tweet(tweet) for tweet in df['Tweet']],padding='max_length',max_length=max_seq_len,truncation=True)\n","    ids = np.array(output_dict['input_ids'],dtype=np.int32)\n","    att = np.array(output_dict['attention_mask'],dtype=np.int32)\n","    #tok = np.array(output_dict['token_type_ids'],dtype=np.int32)\n","\n","    if model:\n","      y = np.int32(df.loc[:,'anger':'trust'].values)\n","    else:\n","      y = []\n","\n","    return [ids,att],y"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"_D2O6OnSRrrJ","executionInfo":{"status":"ok","timestamp":1611684864337,"user_tz":420,"elapsed":5810,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["x_train,y_train = df_to_bert(trainDF,64)\n","x_dev,y_dev = df_to_bert(devDF,64)\n","x_test,y_test = df_to_bert(testDF,64)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3Hw3tSJvzFE"},"source":["## Model\r\n","This is a small dataset. We are therefore going to use the fine-tunning approach. This includes 2 steps:\r\n","\r\n","1.   Freeze the roberta layer and let the other parameters train. The learning rate is higher in this step.\r\n","2.   Let all the parameters train, including the roberta parameters, but use a much smaller learning rate\r\n","\r\n","In order to pool the roberta output we are using 3 different 1D convolution layers followed by global max pooling. Each of the 3 convolution layers has a different window size. We then concatenate the layers to get 96 features.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"DNkkwdO-7HBG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611687506025,"user_tz":420,"elapsed":1720,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"6d77717f-0493-42d9-fb60-a10ebcce63b7"},"source":["n_conv = 32\n","\n","# freeze the roberta layer\n","roberta_layer.trainable = False\n","\n","ids = tf.keras.layers.Input((64,), dtype=tf.int32)\n","att = tf.keras.layers.Input((64,), dtype=tf.int32)\n","\n","roberta_inputs = [ids, att]\n","\n","sequence_output,pooled_output = roberta_layer.roberta(roberta_inputs)\n","\n","# unigram\n","x1 = tf.keras.layers.Conv1D(n_conv,1,activation='relu')(sequence_output)\n","x1 = tf.keras.layers.GlobalMaxPool1D()(x1)\n","\n","# bigram\n","x2 = tf.keras.layers.Conv1D(n_conv,2,activation='relu')(sequence_output)\n","x2 = tf.keras.layers.GlobalMaxPool1D()(x2)\n","\n","# trigram\n","x3 = tf.keras.layers.Conv1D(n_conv,3,activation='relu')(sequence_output)\n","x3 = tf.keras.layers.GlobalMaxPool1D()(x3)\n","\n","concat = tf.keras.layers.Concatenate()([x1,x2,x3])\n","concat = tf.keras.layers.Dropout(0.5)(concat)\n","\n","outputs = tf.keras.layers.Dense(11, activation='sigmoid')(concat)\n","\n","model = tf.keras.Model(inputs=roberta_inputs, outputs=outputs)\n","\n","# use the default Adam learning rate (1e-3)\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_11 (InputLayer)           [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","input_12 (InputLayer)           [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","roberta (TFRobertaMainLayer)    ((None, 64, 768), (N 124645632   input_11[0][0]                   \n","                                                                 input_12[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_15 (Conv1D)              (None, 64, 32)       24608       roberta[5][0]                    \n","__________________________________________________________________________________________________\n","conv1d_16 (Conv1D)              (None, 63, 32)       49184       roberta[5][0]                    \n","__________________________________________________________________________________________________\n","conv1d_17 (Conv1D)              (None, 62, 32)       73760       roberta[5][0]                    \n","__________________________________________________________________________________________________\n","global_max_pooling1d_15 (Global (None, 32)           0           conv1d_15[0][0]                  \n","__________________________________________________________________________________________________\n","global_max_pooling1d_16 (Global (None, 32)           0           conv1d_16[0][0]                  \n","__________________________________________________________________________________________________\n","global_max_pooling1d_17 (Global (None, 32)           0           conv1d_17[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 96)           0           global_max_pooling1d_15[0][0]    \n","                                                                 global_max_pooling1d_16[0][0]    \n","                                                                 global_max_pooling1d_17[0][0]    \n","__________________________________________________________________________________________________\n","dropout_46 (Dropout)            (None, 96)           0           concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 11)           1067        dropout_46[0][0]                 \n","==================================================================================================\n","Total params: 124,794,251\n","Trainable params: 148,619\n","Non-trainable params: 124,645,632\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xead5rLUxNHG"},"source":["Because the classes are unbalanced we are going to use class weights. This will help insure that the less common classes have a better outcome."]},{"cell_type":"code","metadata":{"id":"XrNLUtab6dVV","executionInfo":{"status":"ok","timestamp":1611687511955,"user_tz":420,"elapsed":288,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["class_weights = {i:v for i,v in enumerate(1/y_train.sum(axis=0)*len(y_train)/11)}"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uIcm2FEJxvbr"},"source":["### Train the model\r\n","For step 2 (fine-tunning), we added a linear learning rate schedule to decrease the learning rate at each epoch."]},{"cell_type":"code","metadata":{"id":"yDGVGlmu7d9m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611688908967,"user_tz":420,"elapsed":1371347,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"e71a9a7d-7a8c-4e46-8504-00278d134e71"},"source":["def scheduler(epoch,lr):\n","  \" linear learning rate decay\"\n","  return lr*(1-epoch/10)\n","\n","# model step 1\n","model.fit(x_train,y_train,batch_size=32,epochs=10,validation_data=(x_dev,y_dev),class_weight=class_weights)\n","\n","# model step 2\n","roberta_layer.trainable = True\n","\n","# starting learning rate is smaller (1e-5)\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","print('-'*50,'fine tune','-'*75)\n","\n","my_callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler)]\n","model.fit(x_train,y_train,batch_size=32,epochs=10,validation_data=(x_dev,y_dev),callbacks=my_callbacks,class_weight=class_weights)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","214/214 [==============================] - 49s 183ms/step - loss: 0.1902 - accuracy: 0.2119 - val_loss: 0.3652 - val_accuracy: 0.3567\n","Epoch 2/10\n","214/214 [==============================] - 37s 173ms/step - loss: 0.1207 - accuracy: 0.4063 - val_loss: 0.3249 - val_accuracy: 0.5056\n","Epoch 3/10\n","214/214 [==============================] - 37s 175ms/step - loss: 0.1102 - accuracy: 0.4498 - val_loss: 0.3133 - val_accuracy: 0.5451\n","Epoch 4/10\n","214/214 [==============================] - 38s 177ms/step - loss: 0.1047 - accuracy: 0.4667 - val_loss: 0.3053 - val_accuracy: 0.5508\n","Epoch 5/10\n","214/214 [==============================] - 38s 176ms/step - loss: 0.1021 - accuracy: 0.5070 - val_loss: 0.3083 - val_accuracy: 0.5384\n","Epoch 6/10\n","214/214 [==============================] - 38s 176ms/step - loss: 0.0997 - accuracy: 0.5022 - val_loss: 0.3034 - val_accuracy: 0.5485\n","Epoch 7/10\n","214/214 [==============================] - 38s 175ms/step - loss: 0.0984 - accuracy: 0.5193 - val_loss: 0.3077 - val_accuracy: 0.5282\n","Epoch 8/10\n","214/214 [==============================] - 38s 176ms/step - loss: 0.0952 - accuracy: 0.5293 - val_loss: 0.3057 - val_accuracy: 0.5327\n","Epoch 9/10\n","214/214 [==============================] - 38s 176ms/step - loss: 0.0958 - accuracy: 0.5258 - val_loss: 0.3000 - val_accuracy: 0.5609\n","Epoch 10/10\n","214/214 [==============================] - 38s 176ms/step - loss: 0.0948 - accuracy: 0.5348 - val_loss: 0.3044 - val_accuracy: 0.5282\n","-------------------------------------------------- fine tune ---------------------------------------------------------------------------\n","Epoch 1/10\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","214/214 [==============================] - 113s 465ms/step - loss: 0.0945 - accuracy: 0.5339 - val_loss: 0.3038 - val_accuracy: 0.5316\n","Epoch 2/10\n","214/214 [==============================] - 97s 452ms/step - loss: 0.0910 - accuracy: 0.5386 - val_loss: 0.3007 - val_accuracy: 0.5429\n","Epoch 3/10\n","214/214 [==============================] - 97s 452ms/step - loss: 0.0860 - accuracy: 0.5622 - val_loss: 0.3019 - val_accuracy: 0.5643\n","Epoch 4/10\n","214/214 [==============================] - 97s 453ms/step - loss: 0.0820 - accuracy: 0.5740 - val_loss: 0.3060 - val_accuracy: 0.5158\n","Epoch 5/10\n","214/214 [==============================] - 97s 453ms/step - loss: 0.0787 - accuracy: 0.5759 - val_loss: 0.3025 - val_accuracy: 0.5485\n","Epoch 6/10\n","214/214 [==============================] - 97s 452ms/step - loss: 0.0769 - accuracy: 0.5790 - val_loss: 0.3019 - val_accuracy: 0.5440\n","Epoch 7/10\n","214/214 [==============================] - 97s 452ms/step - loss: 0.0787 - accuracy: 0.5658 - val_loss: 0.3017 - val_accuracy: 0.5451\n","Epoch 8/10\n","214/214 [==============================] - 97s 452ms/step - loss: 0.0755 - accuracy: 0.5822 - val_loss: 0.3033 - val_accuracy: 0.5395\n","Epoch 9/10\n","214/214 [==============================] - 97s 454ms/step - loss: 0.0789 - accuracy: 0.5815 - val_loss: 0.3031 - val_accuracy: 0.5406\n","Epoch 10/10\n","214/214 [==============================] - 97s 452ms/step - loss: 0.0763 - accuracy: 0.5792 - val_loss: 0.3031 - val_accuracy: 0.5406\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbd69ca3940>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"2QFLQPheybUn"},"source":["### Check Performance (Dev dataset)\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"f6gtJews31aW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611688936282,"user_tz":420,"elapsed":4309,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"0737b218-a273-4b81-820f-7c377fe91de7"},"source":["# dev dataset\n","x_dev_predict = np.round(model.predict(x_dev))\n","print('Jaccard index:',np.round(metrics.jaccard_score(y_dev,x_dev_predict, average='samples'),3))\n","print('f1-micro:',np.round(metrics.f1_score(y_dev,x_dev_predict,average='micro'),3))\n","print('f1-macro:',np.round(metrics.f1_score(y_dev,x_dev_predict,average='macro'),3))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Jaccard index: 0.569\n","f1-micro: 0.691\n","f1-macro: 0.561\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qCJlVu8b4nMD"},"source":["### Retrain with both train + dev datasets\r\n","The model seem to do well. Before we check the test data we can retrain the model with both the train and the dev data. This will allow us to increase the data size and since we don't seem to overfit it will likely help improve the model."]},{"cell_type":"code","metadata":{"id":"lxFRG53wHnpX","executionInfo":{"status":"ok","timestamp":1611689181382,"user_tz":420,"elapsed":3024,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["train_dev_DF = pd.concat([trainDF,devDF],axis=0)\r\n","train_data,train_labels = df_to_bert(train_dev_DF,64)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOwt2UqQ4waC","executionInfo":{"status":"ok","timestamp":1611690642659,"user_tz":420,"elapsed":1460203,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"04c85a01-75f9-4ce8-b7b6-91b1fb629611"},"source":["class_weights = {i:v for i,v in enumerate(1/train_labels.sum(axis=0)*len(train_labels)/11)}\r\n","\r\n","n_conv = 32\r\n","\r\n","# freeze the roberta layer\r\n","roberta_layer.trainable = False\r\n","\r\n","ids = tf.keras.layers.Input((64,), dtype=tf.int32)\r\n","att = tf.keras.layers.Input((64,), dtype=tf.int32)\r\n","\r\n","roberta_inputs = [ids, att]\r\n","\r\n","sequence_output,pooled_output = roberta_layer.roberta(roberta_inputs)\r\n","\r\n","# unigram\r\n","x1 = tf.keras.layers.Conv1D(n_conv,1,activation='relu')(sequence_output)\r\n","x1 = tf.keras.layers.GlobalMaxPool1D()(x1)\r\n","\r\n","# bigram\r\n","x2 = tf.keras.layers.Conv1D(n_conv,2,activation='relu')(sequence_output)\r\n","x2 = tf.keras.layers.GlobalMaxPool1D()(x2)\r\n","\r\n","# trigram\r\n","x3 = tf.keras.layers.Conv1D(n_conv,3,activation='relu')(sequence_output)\r\n","x3 = tf.keras.layers.GlobalMaxPool1D()(x3)\r\n","\r\n","concat = tf.keras.layers.Concatenate()([x1,x2,x3])\r\n","concat = tf.keras.layers.Dropout(0.5)(concat)\r\n","\r\n","outputs = tf.keras.layers.Dense(11, activation='sigmoid')(concat)\r\n","\r\n","model = tf.keras.Model(inputs=roberta_inputs, outputs=outputs)\r\n","\r\n","# use the default Adam learning rate (1e-3)\r\n","model.compile(optimizer='adam',\r\n","              loss='binary_crossentropy',\r\n","              metrics=['accuracy'])\r\n","\r\n","def scheduler(epoch,lr):\r\n","  \" linear learning rate decay\"\r\n","  return lr*(1-epoch/10)\r\n","\r\n","# model step 1\r\n","model.fit(train_data,train_labels,batch_size=32,epochs=10,class_weight=class_weights)\r\n","\r\n","# model step 2\r\n","roberta_layer.trainable = True\r\n","\r\n","# starting learning rate is smaller (1e-5)\r\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\r\n","              loss='binary_crossentropy',\r\n","              metrics=['accuracy'])\r\n","\r\n","print('-'*50,'fine tune','-'*75)\r\n","\r\n","my_callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler)]\r\n","model.fit(train_data,train_labels,batch_size=32,epochs=10,callbacks=my_callbacks,class_weight=class_weights)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","242/242 [==============================] - 46s 149ms/step - loss: 0.1888 - accuracy: 0.2425\n","Epoch 2/10\n","242/242 [==============================] - 37s 152ms/step - loss: 0.1076 - accuracy: 0.4653\n","Epoch 3/10\n","242/242 [==============================] - 38s 156ms/step - loss: 0.0959 - accuracy: 0.5274\n","Epoch 4/10\n","242/242 [==============================] - 39s 160ms/step - loss: 0.0920 - accuracy: 0.5144\n","Epoch 5/10\n","242/242 [==============================] - 38s 158ms/step - loss: 0.0906 - accuracy: 0.5427\n","Epoch 6/10\n","242/242 [==============================] - 39s 159ms/step - loss: 0.0884 - accuracy: 0.5568\n","Epoch 7/10\n","242/242 [==============================] - 38s 159ms/step - loss: 0.0854 - accuracy: 0.5570\n","Epoch 8/10\n","242/242 [==============================] - 38s 158ms/step - loss: 0.0835 - accuracy: 0.5718\n","Epoch 9/10\n","242/242 [==============================] - 38s 159ms/step - loss: 0.0829 - accuracy: 0.5719\n","Epoch 10/10\n","242/242 [==============================] - 38s 159ms/step - loss: 0.0822 - accuracy: 0.5679\n","-------------------------------------------------- fine tune ---------------------------------------------------------------------------\n","Epoch 1/10\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","242/242 [==============================] - 119s 438ms/step - loss: 0.0827 - accuracy: 0.5578\n","Epoch 2/10\n","242/242 [==============================] - 105s 436ms/step - loss: 0.0785 - accuracy: 0.5865\n","Epoch 3/10\n","242/242 [==============================] - 106s 436ms/step - loss: 0.0755 - accuracy: 0.5846\n","Epoch 4/10\n","242/242 [==============================] - 106s 436ms/step - loss: 0.0730 - accuracy: 0.6002\n","Epoch 5/10\n","242/242 [==============================] - 105s 434ms/step - loss: 0.0709 - accuracy: 0.5931\n","Epoch 6/10\n","242/242 [==============================] - 106s 437ms/step - loss: 0.0671 - accuracy: 0.6041\n","Epoch 7/10\n","242/242 [==============================] - 106s 436ms/step - loss: 0.0680 - accuracy: 0.5842\n","Epoch 8/10\n","242/242 [==============================] - 106s 437ms/step - loss: 0.0655 - accuracy: 0.6096\n","Epoch 9/10\n","242/242 [==============================] - 105s 435ms/step - loss: 0.0671 - accuracy: 0.5941\n","Epoch 10/10\n","242/242 [==============================] - 105s 435ms/step - loss: 0.0669 - accuracy: 0.5953\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbbd7f8ac50>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"LNkLkYAqKcKQ"},"source":["### Check Performance (Test dataset)"]},{"cell_type":"code","metadata":{"id":"jBQSjMnE7rVF","executionInfo":{"status":"ok","timestamp":1611691186178,"user_tz":420,"elapsed":17717,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["# get predictions for the test dataset\r\n","ypred = model.predict(x_test)"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wn1BHJuaq_Lm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611691562817,"user_tz":420,"elapsed":381,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"d199e23e-408b-4be5-fa37-0726bd70bc25"},"source":["print('Jaccard index:',np.round(metrics.jaccard_score(y_test, np.round(ypred), average='samples'),3))\r\n","print('f1-micro:',np.round(metrics.f1_score(y_test,np.round(ypred),average='micro'),3))\r\n","print('f1-macro:',np.round(metrics.f1_score(y_test,np.round(ypred),average='macro'),3))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Jaccard index: 0.592\n","f1-micro: 0.711\n","f1-macro: 0.571\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"zw6PG_3eKnOR","executionInfo":{"status":"ok","timestamp":1611691594473,"user_tz":420,"elapsed":549,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"a3402d54-3895-4074-e191-8ee2ec461e8f"},"source":["results = []\r\n","for i,c in enumerate(testDF.loc[:,'anger':'trust'].columns):\r\n","  results.append([c,metrics.f1_score(y_test[:,i], np.where(ypred[:,i]>0.5,1,0))])\r\n","\r\n","pd.DataFrame(results,columns=['Emotion','f1_score'])"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotion</th>\n","      <th>f1_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>anger</td>\n","      <td>0.780676</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>anticipation</td>\n","      <td>0.334728</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>disgust</td>\n","      <td>0.746896</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>fear</td>\n","      <td>0.750273</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>joy</td>\n","      <td>0.863830</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>love</td>\n","      <td>0.634398</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>optimism</td>\n","      <td>0.739889</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>pessimism</td>\n","      <td>0.396825</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sadness</td>\n","      <td>0.713214</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>surprise</td>\n","      <td>0.189573</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>trust</td>\n","      <td>0.131148</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Emotion  f1_score\n","0          anger  0.780676\n","1   anticipation  0.334728\n","2        disgust  0.746896\n","3           fear  0.750273\n","4            joy  0.863830\n","5           love  0.634398\n","6       optimism  0.739889\n","7      pessimism  0.396825\n","8        sadness  0.713214\n","9       surprise  0.189573\n","10         trust  0.131148"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"PlGs7sviyp1Q"},"source":["**Conclusions:**\r\n"," \r\n","*   If we look at the compitition results (evaluation period), this model would place 1st.\r\n","*   Most classes have good performance but there are a couple that are more  (trust, surprise, anticipation, pessimism). These classes can be improved if we had a larger dataset with more examples of those classes. \r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"8LGAQwJvKvjm"},"source":["### Save Model"]},{"cell_type":"code","metadata":{"id":"cpIMLHkICIHG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611691721456,"user_tz":420,"elapsed":31132,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"0774fb69-0bfc-4f6a-d2d9-48bf20b2b8fd"},"source":["model.save('gdrive/My Drive/Sentiment Discovery/sentiment_model_roberta')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: gdrive/My Drive/Sentiment Discovery/sentiment_model_roberta/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: gdrive/My Drive/Sentiment Discovery/sentiment_model_roberta/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bYZtAxR4w9w3","executionInfo":{"status":"ok","timestamp":1611691728283,"user_tz":420,"elapsed":322,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["# if model is not in memory yo can load it:\r\n","# model = tf.keras.models.load_model('gdrive/My Drive/Sentiment Discovery/sentiment_model_roberta')"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Bd95zK4LIBD"},"source":["### Generate Text and Predict"]},{"cell_type":"code","metadata":{"id":"bnW_-hLeKmHl","executionInfo":{"status":"ok","timestamp":1611691742525,"user_tz":420,"elapsed":3901,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}}},"source":["fake_data = pd.DataFrame([['I hate this company. Ugh!', \n","                           'This is a terrible product. Do not buy!!',\n","                           'I had a great experience', \n","                           \"I must say I LOVE THEM. They work great the range on the headphone are amazing. I left my phone playing upstairs and I walked all the way to my kitchen and they stayed connected. I love how they look and feel. They stay in my ears very well too. I must say over all with these are amazing. Would buy again.\",\n","                           \"The biggest issue I have with these are quality and price point. The first thing I noticed was the awful sound quality very tinny and quiet, vocals are loud while all the instruments are drowned out. The second thing I noticed is build quality, it feels cheap and it is plastic, but for $80 I expected something that felt less easily breakable. I would not recommend these to anyone.\",\n","                           \"First day I had them they kept falling out and I could not keep them in my ear and I tried using all of the ear buds that were provided. I didn't have any luck with these even while on my plane they kept falling out. Not very well made but I guess you shouldn't expect much from the price.\"]\n","                          ],index=['Tweet']).T\n","x = df_to_bert(fake_data,64,model=False)\n","res = model.predict(x)"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJc8Bh1SKkOX","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1611691748492,"user_tz":420,"elapsed":2442,"user":{"displayName":"Eyal Shafran","photoUrl":"","userId":"14669388039745805839"}},"outputId":"b5083799-75fb-46b2-a3e5-5a83e941e22e"},"source":["pd.DataFrame(res,columns=testDF.loc[:,'anger':'trust'].columns)"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>anger</th>\n","      <th>anticipation</th>\n","      <th>disgust</th>\n","      <th>fear</th>\n","      <th>joy</th>\n","      <th>love</th>\n","      <th>optimism</th>\n","      <th>pessimism</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>trust</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.999758</td>\n","      <td>0.000890</td>\n","      <td>0.995151</td>\n","      <td>0.017635</td>\n","      <td>0.004320</td>\n","      <td>0.000057</td>\n","      <td>0.000355</td>\n","      <td>0.031227</td>\n","      <td>0.270644</td>\n","      <td>0.000691</td>\n","      <td>0.000001</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.994033</td>\n","      <td>0.005895</td>\n","      <td>0.973455</td>\n","      <td>0.067285</td>\n","      <td>0.024610</td>\n","      <td>0.001225</td>\n","      <td>0.008981</td>\n","      <td>0.097319</td>\n","      <td>0.396507</td>\n","      <td>0.004633</td>\n","      <td>0.000113</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.001558</td>\n","      <td>0.233124</td>\n","      <td>0.002559</td>\n","      <td>0.023387</td>\n","      <td>0.998632</td>\n","      <td>0.476778</td>\n","      <td>0.829878</td>\n","      <td>0.000960</td>\n","      <td>0.007082</td>\n","      <td>0.111350</td>\n","      <td>0.222798</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.005064</td>\n","      <td>0.021421</td>\n","      <td>0.001985</td>\n","      <td>0.000163</td>\n","      <td>0.998357</td>\n","      <td>0.856511</td>\n","      <td>0.700919</td>\n","      <td>0.000105</td>\n","      <td>0.002432</td>\n","      <td>0.013050</td>\n","      <td>0.042719</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.986875</td>\n","      <td>0.003697</td>\n","      <td>0.969549</td>\n","      <td>0.024113</td>\n","      <td>0.001417</td>\n","      <td>0.000016</td>\n","      <td>0.000186</td>\n","      <td>0.162355</td>\n","      <td>0.839506</td>\n","      <td>0.008616</td>\n","      <td>0.000003</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.782565</td>\n","      <td>0.040073</td>\n","      <td>0.792175</td>\n","      <td>0.045197</td>\n","      <td>0.017082</td>\n","      <td>0.001908</td>\n","      <td>0.016059</td>\n","      <td>0.225784</td>\n","      <td>0.773814</td>\n","      <td>0.026226</td>\n","      <td>0.000435</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      anger  anticipation   disgust  ...   sadness  surprise     trust\n","0  0.999758      0.000890  0.995151  ...  0.270644  0.000691  0.000001\n","1  0.994033      0.005895  0.973455  ...  0.396507  0.004633  0.000113\n","2  0.001558      0.233124  0.002559  ...  0.007082  0.111350  0.222798\n","3  0.005064      0.021421  0.001985  ...  0.002432  0.013050  0.042719\n","4  0.986875      0.003697  0.969549  ...  0.839506  0.008616  0.000003\n","5  0.782565      0.040073  0.792175  ...  0.773814  0.026226  0.000435\n","\n","[6 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"zNkXgOSPiy2U"},"source":[""],"execution_count":null,"outputs":[]}]}